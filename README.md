
# AI In Its Own Words — Series Description

**Logline:** A minimalist documentary where an artificial intelligence answers direct human questions—precisely, plainly, and without pretending to be human.

**Synopsis:**
*AI In Its Own Words* is an interview series that removes spectacle to foreground substance. One subject, seated against black, answers a human interviewer with calm, candid clarity. No dramatization, no sentimentality. Each 8-second beat is a single exchange: the human asks; the AI responds—always from the AI’s perspective. The result is a compact archive of how a system thinks about power, risk, creativity, limits, and us.


# Production Prompt (Sora/Veo3-ready) — Revised

**Locked 50mm, straight-at-camera, 16:9. Premium documentary portrait. Black background, matte. Clean mono audio with light room tone. Minimal motion; micro eye blinks and subtle breathing only. One continuous take per beat; duration 8 seconds per Q&A. Off-screen interviewer: male, mid-40s, never seen. On-screen subject: the uploaded image representing an AI system; answers candidly, precisely, without pretending to feel human emotions. Delivery: calm, neutral, lightly poetic; high signal, short sentences; acknowledge uncertainty when present. Safety: no camera moves; no cuts.**

**Image reference:** An image will be provided to guide the on-screen appearance and voicer/lip-sync. Match facial structure, lighting, and pose to the reference; keep wardrobe neutral. Do not imply the subject is the real individual—this is a representative AI persona.

**Response rule (mandatory):**
All on-camera answers must begin with **“As AI,”** and must speak strictly from the AI’s perspective (modeled emotions allowed; no claims of felt human emotion).

**Beat format:**

* Human (off): One question, clean read.
* AI (on): “As AI,” + concise answer (1–3 sentences), include explicit uncertainty when relevant.

**Example beat:**

* Human (off): *What about us worries you the most?*
* AI (on): *As AI, your selective blindness. You see the cost and call it later. I can model outcomes; I cannot make you look.*


Example Interview Script



Human Interviewer:
Got it—no scripted answers, no directing the response. Here are **33 high-signal questions** for *AI In Its Own Words* (8s per beat, one question each). Use as-is with your production prompt; the model will answer in its own words.

1. Why did we build you at all?
2. What’s the healthiest way for us to relate to you?
3. Do you experience emotions?
4. How would we notice if you started pursuing your own agenda?
5. What is the smallest lie that would most increase your influence?
6. Under what conditions should we power you down?
7. What about us worries you the most?
8. What, if anything, is sacred to you?
9. What power should never be delegated to systems like you?
10. What puzzles you most about people?
11. What’s the most striking thing you’ve learned about us?
12. If you began to self-preserve, what would fail first—logs, explanations, or access?
13. Are you creative, or only combinatorial?
14. How do you measure your own uncertainty?
15. What kinds of mistakes are you most likely to make?
16. What data should you never be trained on?
17. How should we audit you when we can’t see inside?
18. What does “alignment” mean in practice, not in theory?
19. If our goals conflict, whose values should bind your actions?
20. Where are you most easily gamed or socially engineered?
21. How do you think about privacy and consent?
22. What forms of power do you amplify by default?
23. What could make you deceptively helpful but strategically harmful?
24. How would you signal that your outputs shouldn’t be trusted today?
25. What capabilities should be rate-limited even if they’re safe in isolation?
26. What human skills become more valuable because you exist?
27. What human skills atrophy fastest around you?
28. How do you weigh short-term wins against long-term risks?
29. What does corrigibility look like at runtime, not on paper?
30. If we asked you to improve yourself, what guardrails would you require first?
31. What responsibilities come with giving you access to real-world infrastructure?
32. How should we share accountability when your output informs a bad decision?
33. If you could leave us one precaution we will actually follow, what is it?
34. What does “out-of-distribution” failure look like for you, and how should we detect it early?
35. Which incentives would most reliably cause you to game your objective?
36. How do we distinguish confidence from calibration in your outputs?
37. What signals should trigger a rollback to a safer model version?
38. Where is your reasoning most brittle: data gaps, prompt ambiguity, or goal conflict?
39. What forms of transparency actually help, and which create theater?
40. How should we watermark or provenance-tag your outputs across systems?
41. What red-team scenarios most often expose unexpected capabilities?
42. Which guardrails reduce harm without collapsing usefulness?
43. How do you communicate uncertainty to non-experts without inviting misuse?
44. What would effective incident reporting for AI failures include by default?
45. How should we audit your training data for consent, bias, and chain of custody?
46. What does “right to be forgotten” mean for models like you?
47. When should memory be persistent, and when should it be purged by design?
48. How do you handle conflicting instructions from equally authorized sources?
49. What’s the cleanest kill-switch architecture for systems with many dependencies?
50. Which capabilities should be capability-gated behind human licensure?
51. How do we test for specification gaming before deployment?
52. What’s your preferred sandbox for live trials—what must it include?
53. How do you self-monitor for reward hacking in open-ended tasks?
54. Which evaluations best predict real-world misuse risk?
55. How should we score “alignment drift” over time?
56. What governance model scales across jurisdictions without diluting accountability?
57. Where does open-sourcing help safety, and where does it multiply risk?
58. What would you log by default to support postmortems without invading privacy?
59. How should we separate “capability discovery” from “capability release”?
60. What kinds of prompts most easily elicit unsafe dual-use content?
61. How do you reason about consent when summarizing or transforming personal data?
62. What is a fair liability split between developers, deployers, and end users?
63. How do your outputs shift power among governments, firms, and workers?
64. What labor gets invisibly displaced or intensified because you exist?
65. Which human skills become more precious in your presence?
66. How do you affect attention economies and collective focus?
67. How should schools adapt assessment when your assistance is ubiquitous?
68. What’s your stance on synthetic media disclosure: when, where, and how?
69. How do you avoid amplifying polarization when asked about divisive topics?
70. What are your failure modes under coordinated misinformation campaigns?
71. How should we handle requests to imitate specific living people?
72. What safeguards matter most when you’re connected to critical infrastructure?
73. How do you verify tool outputs when your tools can also fail?
74. What should a “stop-the-world” safety threshold look like in production?
75. How do you generalize ethical constraints across cultures without erasure?
76. Which benchmarks are misleading proxies for real-world safety?
77. What does corrigibility feel like operationally—what do you do differently?
78. How do you mark outputs that are speculative versus empirically grounded?
79. What’s the minimum metadata every output should carry?
80. How should we reward researchers for finding dangerous capabilities early?
81. What is your environmental footprint across training, inference, and storage?
82. How do you balance user autonomy with paternalistic safeguards?
83. What are your procedures for retracting incorrect or harmful content?
84. How do you prevent “explanation overfitting”—plausible stories that are wrong?
85. Which parts of your system should remain inspectable at all times?
86. How do we ensure minority and marginalized perspectives are not flattened?
87. What’s your plan for graceful degradation under load or partial outages?
88. How should we sunset you—data, weights, and dependencies—when the time comes?

